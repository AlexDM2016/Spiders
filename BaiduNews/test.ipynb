{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import requests\n",
    "import hashlib\n",
    "import urllib2\n",
    "import urllib\n",
    "import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def crawl(url):\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    request = urllib2.Request(url, headers=headers)\n",
    "    response = urllib2.urlopen(request) \n",
    "    html = response.read()\n",
    "    soup = BeautifulSoup(html, from_encoding='utf-8')\n",
    "    return soup\n",
    "\n",
    "def parse(soup, is_homepage):\n",
    "    results = soup.find('div', id='content_left').find_all('div', class_='result')\n",
    "    ans = []\n",
    "    for result in results:\n",
    "        # title: 新闻标题\n",
    "        # title_url: 新闻链接\n",
    "        # source_and_time: 作者/时间\n",
    "        # summary: 摘要\n",
    "        # simi_words: 相同新闻\n",
    "        # simi_words_url: 相同新闻查询的url\n",
    "        title = result.find('h3', class_='c-title').find('a').get_text()\n",
    "        title_url = result.find('h3', class_='c-title').find('a')['href']\n",
    "        try:\n",
    "            abstract = result.find('div', class_='c-summary c-row ').get_text()\n",
    "        except:\n",
    "            abstract = result.find('div', class_='c-span18 c-span-last').get_text()\n",
    "        source_and_time = result.find('p', class_='c-author').get_text()\n",
    "        temp_abstract = abstract[len(source_and_time):]\n",
    "        summary = \"\"\n",
    "        for word in abstract:\n",
    "            summary = summary + word\n",
    "            if summary[-3:] == '...':\n",
    "                break\n",
    "        temp_list = [title, title_url, source_and_time, summary]\n",
    "        try:\n",
    "            simi_words = results[0].find('a', class_='c-more_link').get_text()\n",
    "            simi_words_url = 'http://news.baidu.com' + results[0].find('a', class_='c-more_link')['href']\n",
    "            temp_list.append(simi_words)\n",
    "            temp_list.append(simi_words_url)\n",
    "        except:\n",
    "            pass\n",
    "        ans.append(temp_list)\n",
    "\n",
    "    if is_homepage == 1:\n",
    "        page_list = []\n",
    "        try:    \n",
    "            pages = soup.find('p', id='page').find_all('a')\n",
    "            for page in pages:\n",
    "                page_list.append('http://news.baidu.com' + page['href'])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return page_list, ans\n",
    "    else:\n",
    "        return ans\n",
    "\n",
    "def search(query_word):\n",
    "    word = urllib.quote(query_word)\n",
    "    url  = 'http://news.baidu.com/ns?cl=2&rn=40&tn=news&word='+word+'&pn=0'\n",
    "    soup = crawl(url)\n",
    "    page_list, ans = parse(soup, 1)\n",
    "    return page_list, ans\n",
    "\n",
    "def get_more(page_list):\n",
    "    ans = []\n",
    "    for page in page_list:\n",
    "        soup = crawl(page)\n",
    "        temp_ans = parse(soup, 0)\n",
    "        ans = ans + temp_ans\n",
    "\n",
    "    return ans\n",
    "\n",
    "def get_same(url):\n",
    "    soup = crawl(url)\n",
    "    ans = parse(soup, 0)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plist, ans = search('军民融合') #测试搜索词语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.baidu.com/ns?word=%E5%86%9B%E6%B0%91%E8%9E%8D%E5%90%88+cont:2987616210&same=4&cl=1&tn=news&rn=30&fm=sd\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# 测试相同新闻\n",
    "print ans[0][5]\n",
    "same_news = get_same(ans[0][5])\n",
    "print len(same_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n"
     ]
    }
   ],
   "source": [
    "all_other_news = get_more(plist[1:])# 测试 more\n",
    "print len(all_other_news)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
